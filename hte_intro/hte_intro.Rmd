---
title: "Explicitly Optimizing on Causal Effects: A Gentle Introduction to Heterogenous Treatment Effects for Political Messaging Practitioners"
output: html_document
---

In this post, I argue for and demonstrate how to train a model optimized on a treatment's causal effect. This involves predicting the *lift* a treatment is expected to have over the control, which is defined as the difference in an outcome *Y* between treatment and control conditions. This stands in contrast to most supervised learning models, which focus on predicting *Y* and tend to ignore causality. This lack of focus on causality in maching learning has been criticized recently, most notably by Judea Pearl in *The Book of Why: The New Science of Cause and Effect*. One of his gripes with the current big data and machine learning literature (he focuses specifically on "artificial intelligence") is that do not consider causality. If a machine is ever to reach human-level intelligence, Pearl argues, it must understand cause and effect.  

But there is a growing area of research focused on causality in machine learning, which I refer to generally as "heterogeneous treatment effects models." In the last few years, a number of interesting papers have been published on the estimation and prediction of treatment effects. There still exists a gap, however, between those proposing these methods and those implementing and using them. My goal here is to bridge this divide by demystifying these models and showing R code for doing these analyses.  

# Use Cases

Estimating heterogenous treatment effects (HTEs) can be useful when one is interested in targeting individuals for an intervention. In medicine, one might want to find the people who will be most helped (or least harmed) by a drug or therapy. In marketing, one could be interested in retaining subscribers by targeting those people most likely to be swayed by some appeal to stay with the service. The practice of HTEs in these fields is also referred to as personalized medicine and personalized marketing, respectively. But my focus in this post is on the field in which I work: political messaging. Who should we try to persuade to vote for our candidate or to donate money to their campaign? Which doors should we knock on? Which people should we tell to go vote? In all of these examples, we are trying to cause—to bring about—some behavior or attitude change, and the question is: Who should we target with our intervention aimed at causing this behavior?  

The objective here, put broadly, is to maximize lift (i.e., the treatment effect). To do so, we run a randomized experiment; in a given sample, half are randomly assigned to some treatment intervetion, while the other half are assigned to a control or placebo group. I define lift as the difference between the expected outcome in the treatment minus the expected outcome in the control. If our outcome is dichotomous (e.g., yes or no, did vote or stayed home), the lift is the probability of the desired behavior in the treatment minus the probability of the desired behavior in the control. For example, if 55% of the people in the treatment voted, while 50% in the control voted, our lift would be 55% - 50% = 5 percentage points. If our outcome is continuous (e.g., how much money someone donated, how favorably they view our candidate), then the lift is the mean in the treatment minus the mean in the control. For example, if the treatment yielded a \$10.00 average donation and the control yielded \$7.50, then the lift would be \$10.00 - \$7.50 = \$2.50.  

After running the experiment, our goal is to estimate the expected lift for every individual. These estimates may vary widely between different subgroups in the sample, which is where the name *heterogenous* treatment effects stems from. We then deliver our treatment to those who have the biggest expected lift. I discuss some common alternative strategies before discussing methods that estimate HTEs.  

# Other Common Strategies

**Neutral or Uncertain.** This strategy involves targeting people who have neutral or uncertain attitudes on some issue and delivering them persuasion messages. Those attempting to persuade people that their candidate is qualified, for example, might choose to target people who said "Don't Know" or "Neither Qualified nor Unqualified" to the question: "Do you find Candidate X to be qualified?" The idea would be to survey a sample on this question, train a machine learning model to predict what types of people give these responses, and then target those people.  

**Importance.** Find what people are passionate about and deliver them messages about those passions. A good example for this is a get out the vote (GOTV) operation. Similar to the above strategy, one would conduct a survey on a number of issues, pinpoint the issues people say they agree with and are very important to them, train a model to score likelihood of being in agreement with and passionate about an issue, and then deliver a message focused on how voting affects that issue to people scoring high on the model.  

**Personas.** A third strategy might be to classify people into *k* groups and tailor messages to what makes these groups unique. In a previous post, I talked about how one could survey people who identify as Democrats, cluster them into groups based on their attitudes toward 18 different issues, and then develop messages that adhere to what makes those groups unique. Does one group of Democrats have much stronger attitudes about immigration? Target those people with immigration-related messages. One group is very liberal on every issue? Target those people with the most progressive messages, and so on.  

## Optimization Problems

The problem with each of these three approaches is that **they do not optimize directly on what we are interested in—the lift.** This means that we may have one goal in mind (maximizing lift), while the above approaches optimize on a separate goal. It may very well be that the two goals overlap at times, but there are likely many times where they do not. In any statistical learning problem, it's important to know what an algorithm is optimizing on and making sure the goals of that procedure align with what you are interested in.  

So what do the above approaches optimize on, if not the lift? Keep in mind that the lift is the difference between our outcome *Y* in the treatment and control conditions: *Y*(Treatment) - *Y*(Control).  

- *Neutral or Uncertain.* We might define this by finding the middle portion of the distribution in the pre-score (or modeled score) of an outcome of interest *Y* and then targeting people with scores in that range. Let's say *L* is a lower cutoff—it is some number where the people below have "strongly" negative attitudes—and *U* is the upper cutoff, where people above this number have "strongly" positive attitudes. This approach would be implicitly optimizing on: *L* < *Y* < *U*. This may be particularly unhelpful if people opt for "neutral" or "don't know" options as a way to opt-out of answering the question at all—not because they don't actually have a view. In this case, one would actually be optimizing on something completely different: A person's likelihood to not want to give an answer on a questionnaire.  

- *Importance.* We could define this by finding people who both (a) strongly support an issue and (b) say the issue is very important to them (again, based on pre-scores or modeled ones). Let's call *S* how much someone supports the issue and *I* how important it is to them. We could again define an upper cutoff *U* where people above this threshold strongly support the issue and say it is very important. The behavior we are interested in *Y* might be donations to a campaign. We would be implicitly optimizing on: *U* < *S* and *U* < *I*, where *S* and *I* are assumed to be related to the main outcome *Y*.  

- *Personas.* Unsupervised approaches generally take *p* number of variables and a user-input *k* number of groups, then maximize the variance between groups and/or minimize the variance within the *k* groups on these *p* variables. Other algorithms, like my favorite DBSCAN, use a non-parametric procedure to group points together that are nearby in *p*-dimensional space. Note that some measure of *Y* may or may not be present in the variables used for clustering.  

Each of these approaches are face valid and can provide some benefit at times. However, they do not explicitly model what we are interested in—that is, how much someone will be influenced by our treatment.  

# Finding Heterogenous Treatment Effects

The above strategies do not optimize explicitly on causal effects, but there are some models that do. Before surveying some approaches for estimating HTEs, I discuss some important theoretical background.  

## Theoretical Background

The goal is to estimate the causal effect for an individual: *Y*(Treatment) - *Y*(Control). Most papers I've read proposing, implementing, or reviewing algorithms that do this generally frame this problem within the Rubin causal model (a.k.a., potential outcomes model). This approach defines causality strictly: It is the value we would have observed if a person was in the treatment, minus what we would have observed if a person was in the control. There's a problem here, since a person can only be in one condition at a time. If someone is assigned to the control condition, we never observe the outcome if they had been in the treatment condition (and vice versa). Note that this is why the "would have" language was used, which highlights that these outcomes are "potential."  

Under this framework, getting the actual causal effect requires literal magic: We take our world, split it into two universes, have an individual in Universe 1 go through the treatment and that same person in Universe 2 go through a placebo, then compare the outcomes from Universes 1 and 2. This impossibility is what Holland referred to as "the fundamental problem of causal inference."  

So what do we do? One solution is to get two large groups of people that are similar to one another, assign one of these groups to the treatment, the other to the control, and then compare the expected outcome (e.g., mean response or probability of a behavior occurring) between the two groups. This gives us the average treatment effect (ATE)—the lift across all people in the sample. When I say that the two groups are "similar," I mean that any miscellaneous characteristics about these people that could influence either (a) what treatment they saw or (b) their potential outcomes *Y*(Treament) and *Y*(Control) have been accounted for. The gold standard for doing this is conducting a randomized experiment, where people are chosen at random to be either in the control or treatment condition. Since I am only going to consider estimating HTEs within the realm of an experiment, I don't discuss ensuring this assumption (called "strong ignorability") further.  

The estimated ATE described above is across all people. We want to know the treatment effect for *a given individual* so that we can decide whether or not to target them with our treatment if their estimated treatment effect is in the strength and of the direction desired. But we can only ever measure *Y* in the treatment *or* control—so what do we do? Crudely put, all of the algorithms I mention below more or less search out *similar others* and use them as stand-ins for a given person's potential outcome. For example, imagine the only variables researchers had were race, gender, generation, political affilition, and education. My estimated treatment effect would be the mean outcome for White, male, millenial, Democrats with a graduate degree in the treatment condition minus the mean outcome for this subgroup in the control condition. I hope this demystifies the process a little bit. No matter how complicated equations may get in a HTE paper, this is generally what authors are trying to do: Compare the outcomes of people with similar covariate profiles in different conditions and use these differences as estimates of a treatment effect.  

## Heterogenous Treatment Effects: An Overview




# Causal Forest




# A Worked Example




# Conclusion




