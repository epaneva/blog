---
output: html_document
---

How do we obtain valid confidence intervals for weighted survey estimates? It is not as simple as it sounds. Andrew Gelman (2007) famously wrote that "survey weighting is a mess...  standard errors are tricky even with simple weighted means" (p. 153), and this can still feel like the case—especially given the varied use of the word "weights" in statistics. Many different types of weights exist: Frequency weights indicate how many observations have a given covariate profile, inverse variance weights are used to calculate unbiased estimates in weighted least squares, and survey weights can come in many varieties and calculations.  

Design weights are survey weights that derive from the structure of how observations were sampled. For example, one may increase the probability of a minority group being sampled so that accurate estimates about this group can be calculated (Valliant, Dever, & Kreuter, 2013, p. 43); when making inferences to the population as a whole, one would then calculate weights to shrink this group size to the proportion that they are in the population. More complex survey designs, such as two-stage cluster sampling, might weight on the probability of an observation's cluster being sampled and then on the probability of an observation within that cluster being sampled. In these cases, the weight is then the inverse of the probability that an observation was sampled (Lumley, 2010).  

The focus of this paper, however, is on the use of rake weights for online surveys where we sample everyone in a finite population. The weighting we do will be to correct for non-response, not to address the sampling design. Horvitz and Thompson (1952) define a finite universe as one where "we can identify the individual elements" (p. 663); in the current example, imagine that we have a list of everyone in the target population of interest. This is often the case when, for instance, we are trying to survey an audience list of people we think are amenable to buying our product or are open to supporting our candidate. It could also be the case when we are interested in the opinions of everyone in the directory at a given organization. It can be feasible to sample the entire finite population online by sending everyone a link to the survey in an e-mail, message, or digital advertisement. The sampling design is straightforward, as everyone on the list has a sampling probability of 1: We send surveys to everyone. However, response rates for online surveys tend to be very low, opening us up to risk of non-response bias. We use Monte Carlo simulation methods in this paper to examine how to obtain valid confidence intervals for weighted estimates using rake weights.  

## Raking

Raking is also known as "iterative proportional fitting." The "proportional fitting" aspect comes from making adjustments so that the proportion of people of a given group in the respondent data matches that of the population. If the population was 50% women but women only made up 30% of our respondents, then we could increase the weights of women respondents so that they contribute 50% when calculating estimates. An issue arises, however, if we want to look at many auxiliary variables. This would require calculating proportions at every possible combination of the variables. But there may be very small amounts of people—or none—at, for example, each income-by-gender-by-education-by-race-by-age combination. What we can do instead is make the process "iterative": We start with one variable, calculate weights to make the proportions match for that variable, then move to another variable and do the same. This approach does each grouping variable one-by-one until the weights stop changing. This is one of the most common methods of addressing for non-response (Baker et al., 2010, 2013; Mercer, Lau, & Kennedy, 2018). We will be using the `rake` function from the `survey` R package (Lumley, 2004, 2008, 2010), which calls the package's `postStratify()` function repeatedly until convergence.  

The goal is to weight on every auxiliary variable that is correlated with both (a) the outcome variable of interest and (b) the probability of responding (Kalton & Flores-Vervantes, 2003, pp. 91-92). Imagine we are interested in the proportion of people who feel favorably toward the president. We do not need to worry about variables that are related to non-response, but completely independent of favorability (the outcome variable of interest); people who use the Safari internet browser might be less likely to respond than those who use Firefox, but this is not a problem if browser choice is unrelated to favorabiltiy. Similarly, we do not need to worry about variables related to the outcome but not to probability of responding; Republicans will feel more favorably toward the president, but if they are equally as likely to respond to our survey as Democrats and inependents, then this is not a problem. Weighting on variables related to non-response and the outcome eliminates bias in point estimates (Baker, 2013, p. 23). But what about standard errors?  

## Problems

Point estimates are not enough for making meaningful inferences—we also need to quantify our uncertainty. A common way to do this is through 95% confidence intervals (CIs), which represent a range of plausible values around the estimate that could be the true value in the entire population. A more formal definition will be useful for our purposes, however. If we were to do the same survey an arbitrarily large number of times, then 95% of the 95% CIs would include the true population value (Kline, 2004, pp. 26-30). When we do simulation studies, we are repeating the same simulated survey thousands of times; we know that standard errors are being calculated correctly when 95% of our 95% CIs contain the actual population value.  

Imagine you have just calculated rake weights or a column in a dataset given to you contains rake weights. You want to use R to calculate what proportion of people have a value of `1` (as opposed to `0`) for a dichotomous variable `y`, and you would like a standard error around this. What do you do?  

One has to be careful when using weights in R. Different modeling functions can have `weights` arguments that refer to different types of weights. Don't assume that a function's definition of `weights` matches what you are thinking of at the moment.  

Lumley (2010) notes: "Most statistical software... will assume that weights are precision weights or frequency weights" and that this "will often (but not always) give correct point estimates, but will usually give seriously incorrect standard errors, confidence intervals, and *p*-values (p. 5)." Since we are modeling a dichotomous variable in this situation, our first thought might be to use the `glm` function with the `binomial("logit")` link. However, the `?glm` documentation tells us that: "For a binomial GLM prior weights are used to give the number of trials when the response is the proportion of successes..." What does this mean? Let's simulate some data to take a look.  

Imagine we have an experiment with 200 participants. When in the experimental condition (`x == 1`), people have a 70% probability of responding positively to our treatment (e.g., voting, donating, purchasing); when in the control (`x == 0`), people have a 30% probability of doing so. When `y == 1`, it is a positive outcome; it is not when `y == 0`. The data might look like this:  

```{r glm_weights_data, message = FALSE, warning = FALSE}
library(tidyverse)
set.seed(1839)
n <- 200
x <- rbinom(n, 1, .5)
y <- rbinom(n, 1, ifelse(x == 1, .7, .3))
tibble(x, y)
```

We can get the proportion of positive outcomes (what the `glm` documents call "proportion of successes") by grouping on `x` and calculating the mean of `y`:  

```{r glm_weights_table}
(trials_table <- tibble(x, y) %>% 
  group_by(x) %>% 
  summarise(n_trials = n(), prop_successes = mean(y)))
```

The `n_trials` column is what `glm` is expecting for weights—the number of trials (i.e., observations) that the proportion of successes (i.e., positive outcomes) is based on. We can see that supplying this `trials_table` with weights is equal to using the raw data of `x` and `y` to about 5 or 6 decimal places:  

```{r comparing_glms}
(t1 <- summary(glm(prop_successes ~ x, binomial, trials_table, weights = n_trials))$coef)

(t2 <- summary(glm(y ~ x, binomial))$coef)

all.equal(t1, t2)
```

The rake weights we are interested in do *not* refer to the number of trials, and Lumley (2010) argues that the standard errors will be seriously wrong. We ran a simulation experiment to answer two questions: (a) Does using `glm` and giving rake survey weights calculate standard errors incorrectly? and (b) If so, what R functions can we use to calculate accurate standard errors?  

# Methods

## Simulation

The simulation design is pictured in the figure below. Each rectangle box represents a fit model, and this process was repeated 10,000 times. The finite population was set to an *N* = 5,000 for each simulation. Two auxiliary variables, *X~1~* and *X~2~*, were drawn from a Bernoulli distribution with a probability of .5; that is, each *X* was a dichotomous variable, with each observation having an equal chance of being 0 or 1. *X~1~* and *X~2~* are always simulated as independent of one another—they are uncorrelated. Two regression coefficients, one for each *X* variable, were drawn from independent uniform distributions ranging from 0 to 1.5. The *X* variables incremented the log odds of the outcome variable of interest *Y* = 1 at the rate of their corresponding coefficient. Lastly, each realization of the *Y* variable was sampled from a Bernoulli distribution where the probabiltiy for each case was transformed from their log odds, mapping onto the assumptions of a logistic regression.  

```{r sim_flow, echo = FALSE, out.width = '70%', fig.align = 'center'}
knitr::include_graphics("simulation_flow.jpg")
```

The first model was fit on the entire population, `glm(y ~ 1, binomial, population_data)`. In all models, this intercept-only binomial logistic regression was fit (`y ~ 1`), which calculates (in logits) the proportion of cases where *Y* = 1. The population intercept coefficient served as ground truth. Standard errors in each of the following models will be assessed by how frequently the 95% CI includes this population ground truth. According to the definition of a 95% CI, we will consider standard error calculation methods to be faithfully representing uncertainty if 95% of the CIs contain this population estimate.  

The *X* variables form four groups: *X~1~* = *X~2~* = 1; *X~1~* = *X~2~* = 0; *X~1~* = 0 and *X~2~* = 1; *X~1~* = 1 and *X~2~* = 0. A random sample was simulated from this population data, where every case in each of these groups had a 2% probability of "responding." A model was fit using `glm(y ~ 1, binomial, random_sample)`. This simulates what occurs when non-response bias is *absent*.  

A sample with non-response bias was then drawn from the population data. If *X~1~* = *X~2~* = 1, the response probabiltiy was set to 6.5%, while the rest of the groups were set to 0.5%. Since the coefficients determining the population values could not be negative, the observations sampled at 6.5% are always more likely to have *Y* = 1; we will expect the non-response to overestimate the estimate. A model was fit to this sample, `glm(y ~ 1, binomial, biased_sample)`, to demonstrate a baseline of how the estimate would be biased if no weighting were done.  

These data with non-response bias were then converted to a survey design object in the `survey` package by using `dat_design <- svydesign(~ 1, data = biased_sample)`. Rake weights were calculated by passing this survey design object to `rake()`. The weighting targets were calculated from the population data frame with all 5,000 cases; this thus assumes a situation where we have marginal proportions for the entire finite universe. These weights were assigned to a variable named `wts` in the biased sample data. Three models were fit using these data.  

Weights were passed to the `glm` command as a way of assessing Lumley's (2010) contention that standard errors will be incorrect: `glm(y ~ 1, binomial, biased_sample, wts)`. The `survey` package also includes its own generalized linear models function that employs a sandwich-type estimator for standard errors (see Lumley & Scott, 2017, p. 269). We fit a model using this method, `svyglm(y ~ 1, dat_design, family = binomial)`, to test if this estimator provides a valid 95% CI (note that the `dat_design` object contains the rake weights, so this was a weighted model).  

We passed the `dat_design` object to `as.svrepdesign()` with `type = "bootstrap"` specified; the resulting object was used with `svyglm()` to calculate bootstrapped standard errors. These are known as "replicate" weights in survey literature (Groves et al., 2009, Ch. 10; Lumley, 2010, Ch. 2). Bootstrapping involves drawing *R* resamples from the sample, with replacement, and calculating the estimates for each resample. The standard deviation of the estimates across resamples is then considered to be the standard error of the estimate. We employed the `as.svrepdesign()` default *R* = 50 here. See Canty and Davison (1999) for the specific bootstrapping implementation used here, and Kline (2004, Ch. 9) for a broader introduction to resampling methods, including boostrapping.  

For each simulation iteration, we calculated the estimate for each model and a logical value for all models (save for the population model) indicating whether or not the 95% CI for the estimate contained the population value:  

```{r, message = FALSE}
results <- read_csv("sim_results.csv")
head(results)
```

# Results and Conclusion

The only model that did not approximate the population estimate in the long-run was the model calculated with the non-response bias and _no_ weighting (see table below), supporting Lumley's (2010) contention that weighted estimates—regardless of standard error calculation—will tend to be unbiased. All weighted models also showed the exact same estimates.  

```{r}
results %>% 
  select_if(is.double) %>% 
  colMeans() %>% 
  as.matrix()
```

But we do not have the luxury of having the long-run average in applied research—we have just one of these iterations. We would like to use a method that has a narrow spread—where we rarely stray too far away from the population value. Pictured below are density plots of estimates over all 10,000 simulations. The top panel shows how much the estimate varied in the actual population values, while the panel directly below it shows what we see from a sample drawn randomly from it, with no non-response bias. The bottom three distributions are from the weighted models, and again we see that they are all the same—what will differ only are their calculated standard errors.  

We can also see that the spread for the weighted estimates is wider than for the random sample, which shows us that we need a standard error calculation that appreciates this spread. If we use a standard error calculation that assumes the distribution looks like the `random` one (with no non-response bias), we will underestimate our standard errors, 95% CIs, and *p*-values.  

```{r}
results %>% 
  select_if(is.double) %>% 
  gather() %>% 
  mutate(key = gsub("_est", "", key),
         key = factor(key, c("pop", "random", "nonrandom", 
                             "wtdglm", "wtdsvy", "wtdrep"))) %>% 
  ggplot(aes(x = value, fill = key)) +
  geom_density(alpha = .5) +
  facet_grid(key ~ .) +
  theme(legend.position = "none")
```

Are the CIs properly characterizing uncertainty? Remember: If we run a survey a large number of times, 95% of the 95% CIs should contain the true population estimate. We can test if this is the case by testing the coverage of each model—what percentage of the time did the 95% CI cover the population estimate?  

```{r}
results %>% 
  select_if(is.logical) %>% 
  colMeans() %>% 
  as.matrix()
```

The random sample coverage is about 95%, which is what we would expect from taking a sample from the population where the response probability does not depend on any predictors of the outcome of interest. The model calculated from the biased data and with no weighting only includes the population estimate about 36% of the time, but this is what we would expect from a model that is getting an incorrect estimate. What of the weighted models that yield unbiased estimates?  

Including weights in the `glm()` function as if they were frequency weights yields what Lumley (2010) warned: Standard errors that are "seriously incorrect." There was only about 68% coverage, meaning that these 95% CIs were actually only 68% CIs. Using the `svyglm()` function increased this to 90% coverage, but this still means that the 95% CI is not correctly disclosing the uncertainty at hand. Only when we used the 50 bootstrap resamples did it approach 95%, where the weighted replicate model had about 93% coverage.  

From these results, our recommendation would be to calculate 95% CIs using code like:  

```{r, eval = FALSE}
dat_design <- svydesign(~ 1, data = dat) # convert data to survey design
dat_design <- rake(
  dat_design, 
  sample.margins = list(~V1, ~V2), # list of formulas specifying raking variables
  population.margins = pop_targets # list of data frames showing target counts
)
dat_design <- as.svrepdesign(dat_design, type = "bootstrap") # bootstrap
model <- svyglm(y ~ 1, dat_design, family = binomial) # run model
```

The full simulation code can be found on [GitHub](https://github.com/markhwhiteii/blog/blob/master/weights_troubles/weighting_simulations.R).  

## Limitations

Even though the confidence intervals still tended to underestimate uncertainty, these simulations were still quite optimistic. They represented a best case scenario, in various ways. First, there was no error in measuring the variables on which we raked—they were perfect measurements. Second, we had the exact population estimates—this is generally the case when we have a pre-defined list of people we want to survey, but sometimes only estimated population totals are available. Third, we were weighting on exactly the variables—and only the variables—that relate to both non-response and outcome. In applied situations, these assumptions (particularly the first and third) will not hold, and thus we argue people should be even less confident that 95% CIs are valid. In a weighted survey context, we should be especially skeptical when looking at CIs.  

## References


