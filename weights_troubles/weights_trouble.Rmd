---
output: html_document
---

# Outline
- Start with Gelman "weighting is a mess" quote  
- Many different types of weights (survey, frequency, precision); within survey weights, they can be calculated based on different things  
  + Design-based weights used traditionally for surveys, when people doing probability samples where people know the probability that a person was actually in the sample. Weighting is the inverse of probability of being sampled  
- Online surveys only getting more popular, so focusing on these more opt-in non-probability type samples. Two common situations for us:  
  + Complete opt-in sample. Posting a link in an ad or email or website to a group of people that we don't know the information about; we don't know the probability of each person in the population to take the survey  
  + Finite population. This is having a pre-set list of people, an audience, that we want to survey. Everyone will be targeted to get a survey, such as in an ad, so everyone has a probability of 1 of being in the sample. Only issue here is that, for technical reasons, people might not get served (they don't log-on in the sampling period, they have an ad blocker, something goes wrong on the tech side we don't know about)—these are problematic, but we'll assume its random for the purpose of this paper  
- We will focus on online surveys. Instead of design-based weights, we will be focusing on post-stratification weights to address unit non-response  
  + Define post-stratification weights—"post" because done after the sampling process, so not products of the design  
  + Define rake weighting (i.e., repeated post-stratification)  
- The situation we focus on is finite population: We have a list of people we want to survey, but we want to address unit non-response by weighting on auxiliary variables that correlate with both (a) the outcomes of interest and (b) whether or not someone responses  
- Since many different types of weights, different R functions mean different things by saying `weights`. We look at the consequences of giving the wrong types of weights on standard errors  
  + Include quotes from Lumley and others on how point-estimates will be fine mostly, but standard errors will be wrong  
- Background on the `weights` argument in `glm()` (already written about below, after "Start Content...")  
- Simulation study background  
  + Best case scenario: Perfect measurement of what predicts y and dropout, measure all variables that do  
  + Explain simulation superparameters, how sampling worked, etc. (see powerpoint slide)  
  + Explain different techniques—they are technically for more complicated sampling designs, but can be applied here; Lumley talks about how weights are first calculated on the design and probability of sampling, then can be adjusted more with post-stratification; we are going to just assume everyone had equal probability of appearing in sample, given our finite population (audience);  survey package usually used for stratified and cluster sampling—"complex survey designs"  
  + Talk about comparisons: population, random sample, nonrandom, nonrandom + rake in glm, nonrandom + rake in svyglm, nonrandom + rake in svyglm with bootstrapping  
  + Will look at point estimate accuracy as well as 95% CI coverage, which is related to the standard error obviously  
- Go through results  
  + Point estimates  
  + Density plots  
  + Confidence interval coverage  
- Conclusions  
  + Bootstrap!  

# Start Content...

```{r setup, include = FALSE}
library(tidyverse)
results <- read_csv("sim_results.csv")
```

One has to be careful when using weights in R, however, as different modeling functions can have `weights` arguments that refer to different types of weights, so don't assume that the function's definition of `weights` matches what you are thinking of at the moment.  

Lumley (2010) notes: "Most statistical software... will assume that weights are precision weights or frequency weights" and that this "will often (but not always) give correct point estimates, but will usually give seriously incorrect standard errors, confidence intervals, and *p*-values (p. XXXX)." Since we are modeling a dichotomous variable in this situation, we will want to use `glm` with the `binomial("logit")` link function. The `?glm` documentation tells us that: "For a binomial GLM prior weights are used to give the number of trials when the response is the proportion of successes..." What does this mean?  

Imagine we have an experiment with 200 participants. When in the experimental condition (`x` = 1), people have a 70% probability of responding positively to our treatment (e.g., voting, donating, purchasing); when in the control (`x` = 0), people have a 30% probability of doing so. When `y` = 1, it is a positive outcome; it is not when `y` = 0. The data might look like this:  

```{r glm_weights_data}
set.seed(1839)
n <- 200
x <- rbinom(n, 1, .5)
y <- rbinom(n, 1, ifelse(x == 1, .7, .3))
tibble(x, y)
```

We can get the proportion of positive outcomes (what the `glm` documents call "proportion of successes") by grouping on `x` and calculating the mean of `y`:  

```{r glm_weights_table}
(trials_table <- tibble(x, y) %>% 
  group_by(x) %>% 
  summarise(n_trials = n(), prop_successes = mean(y)))
```

The `n_trials` column is what `glm` is expecting for weights—the number of trials (i.e., observations) that the proportion of successes (i.e., positive outcomes) is based on. We can see that supplying this `trials_table` with weights is equal to using the raw data of `x` and `y` to about 5 or 6 decimal places:  

```{r comparing_glms}
(t1 <- summary(glm(prop_successes ~ x, binomial, trials_table, n_trials))$coef)

(t2 <- summary(glm(y ~ x, binomial))$coef)

all.equal(t1, t2)
```

The post-stratification survey weights we are interested in do *not* refer to the number of trials. Lumley (2010) argues that the standard errors will be seriously wrong. I ran a simulation experiment to answer two questions: (a) Does using `glm` and giving post-stratification survey weights calculate standard errors incorrectly? and (b) If so, what R functions can we use to calculate accurate standard errors?  

# Method

```{r}
results %>% 
  select_if(is.double) %>% 
  colMeans() %>% 
  as.matrix()

results %>% 
  select_if(is.double) %>% 
  gather() %>% 
  mutate(key = gsub("_est", "", key),
         key = factor(key, c("pop", "random", "nonrandom", 
                             "wtdglm", "wtdsvy", "wtdrep"))) %>% 
  ggplot(aes(x = value, fill = key)) +
  geom_density(alpha = .5) +
  facet_grid(key ~ .) +
  theme(legend.position = "none")

results %>% 
  select_if(is.logical) %>% 
  colMeans() %>% 
  as.matrix()
```
