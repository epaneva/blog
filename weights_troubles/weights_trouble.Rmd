---
output: html_document
---

How do we obtain valid confidence intervals for weighted survey estimates? It is not as simple as it sounds. Andrew Gelman (2007) famously wrote that "survey weighting is a mess...  standard errors are tricky even with simple weighted means" (p. 153), and this can still feel like the case—especially given the varied use of the word "weights" in statistics. Many different types of weights exist: Frequency weights indicate how many observations have a given covariate profile, inverse variance weights are used to calculate unbiased estimates in weighted least squares, and survey weights can come in many varieties and calculations.  

Design weights are survey weights that derive from the structure of how observations were sampled. For example, one may increase the probability of a minority group being sampled so that accurate estimates about this group can be calculated (Valliant, Dever, & Kreuter, 2013, p. 43); when making inferences to the population as a whole, one would then calculate weights to shrink this group size to the proportion that they are in the population. More complex survey designs, such as two-stage cluster sampling, might weight on the probability of an observation's cluster being sampled and then on the probability of an observation within that cluster being sampled. In these cases, the weight is then the inverse of the probability that an observation was sampled (Lumley, 2010).  

The focus of this paper, however, is on the use of rake weights for online surveys where we sample everyone in a finite population. The weighting we do will be to correct for non-response, not to address the sampling design. Horvitz and Thompson (1952) define a finite universe as one where "we can identify the individual elements" (p. 663); in the current example, imagine that we have a list of everyone in the target population of interest. This is often the case when, for instance, we are trying to survey an audience list of people we think are amenable to buying our product or are open to supporting our candidate. It could also be the case when we are interested in the opinions of everyone in the directory at a given organization. It can be feasible to sample the entire finite population online by sending everyone a link to the survey in an e-mail, message, or digital advertisement. The sampling design is straightforward, as everyone on the list has a sampling probability of 1: We send surveys to everyone. However, response rates for online surveys tend to be very low, opening us up to risk of non-response bias. We use Monte Carlo simulation methods in this paper to examine how to obtain valid confidence intervals for weighted estimates using rake weights.  

## Raking

Raking is also known as "iterative proportional fitting." The "proportional fitting" aspect comes from making adjustments so that the proportion of people of a given group in the respondent data matches that of the population. If the population was 50% women but women only made up 30% of our respondents, then we could increase the weights of women respondents so that they contribute 50% calculating estimates. An issue arises, however, if we want to look at many auxiliary variables. This would require calculating proportions at every possible combination of the variables. But there may be very small amounts of people—or none—at, for example, each income-by-gender-by-education-by-race-by-age combination. What we can do instead is make the process "iterative": We start with one variable, calculate weights to make the proportions match for that variable, then move to another variable and do the same. This approach does each grouping variable one-by-one until the weights stop changing. This is one of the most common methods of addressing for non-response (Baker et al., 2010, 2013; Mercer, Lau, & Kennedy, 2018). We will be using the `rake` function from the `survey` R package (Lumley, 2004, 2008, 2010), which calls the package's `postStratify()` function repeatedly until convergence.  

The goal is to weight on every auxiliary variable that is correlated with both (a) the outcome variable of interest and (b) the probability of responding. Imagine we are interested in the proportion of people who feel favorably toward the president. We do not need to worry about variables that are related to non-response, but completely independent of favorability (the outcome variable of interest); people who use the Safari internet browser might be less likely to respond than those who use Firefox, but this is not a problem if browser choice is unrelated to favorabiltiy. Similarly, we do not need to worry about variables related to the outcome but not to probability of responding; Republicans will feel more favorably toward the president, but if they are equally as likely to respond to our survey as Democrats and inependents, then this is not a problem. Weighting on variables related to non-response and the outcome eliminates bias in point estimates (Baker, 2013, p. 23). But what about standard errors?  

## Problems: Software, Standard Errors

Point estimates are not enough for making meaningful inferences—we also need to quantify our uncertainty. A common way to do this is through 95% confidence intervals (CIs), which represent a range of plausible values around the estimate that could be the true value in the entire population. A more formal definition will be useful for our purposes, however. If we were to do the same survey an arbitrarily large number of times, then 95% of the 95% CIs would include the true population value (Kline, 2004, pp. 26-30). When we do simulation studies, we are repeating the same simulated survey thousands of times; we know that standard errors are being calculated correctly when 95% of our 95% CIs contain the actual population value.  

Imagine you have just calculated rake weights or a column in a dataset given to you has a column for rake weights. You want to use R to calculate what proportion of people have a value of `1` (as opposed to `0`) for a dichotomous variable `y`, and you would like a standard error around this. What do you do?  

One has to be careful when using weights in R, as different modeling functions can have `weights` arguments that refer to different types of weights, so don't assume that the function's definition of `weights` matches what you are thinking of at the moment.  

Lumley (2010) notes: "Most statistical software... will assume that weights are precision weights or frequency weights" and that this "will often (but not always) give correct point estimates, but will usually give seriously incorrect standard errors, confidence intervals, and *p*-values (p. 5)." Since we are modeling a dichotomous variable in this situation, our first thought might be to use the `glm` function with the `binomial("logit")` link. However, the `?glm` documentation tells us that: "For a binomial GLM prior weights are used to give the number of trials when the response is the proportion of successes..." What does this mean? Let's simulate some data to take a look.  

Imagine we have an experiment with 200 participants. When in the experimental condition (`x == 1`), people have a 70% probability of responding positively to our treatment (e.g., voting, donating, purchasing); when in the control (`x == 0`), people have a 30% probability of doing so. When `y == 1`, it is a positive outcome; it is not when `y == 0`. The data might look like this:  

```{r glm_weights_data, message = FALSE, warning = FALSE}
library(tidyverse)
set.seed(1839)
n <- 200
x <- rbinom(n, 1, .5)
y <- rbinom(n, 1, ifelse(x == 1, .7, .3))
tibble(x, y)
```

We can get the proportion of positive outcomes (what the `glm` documents call "proportion of successes") by grouping on `x` and calculating the mean of `y`:  

```{r glm_weights_table}
(trials_table <- tibble(x, y) %>% 
  group_by(x) %>% 
  summarise(n_trials = n(), prop_successes = mean(y)))
```

The `n_trials` column is what `glm` is expecting for weights—the number of trials (i.e., observations) that the proportion of successes (i.e., positive outcomes) is based on. We can see that supplying this `trials_table` with weights is equal to using the raw data of `x` and `y` to about 5 or 6 decimal places:  

```{r comparing_glms}
(t1 <- summary(glm(prop_successes ~ x, binomial, trials_table, weights = n_trials))$coef)

(t2 <- summary(glm(y ~ x, binomial))$coef)

all.equal(t1, t2)
```

The rake weights we are interested in do *not* refer to the number of trials. Lumley (2010) argues that the standard errors will be seriously wrong. We ran a simulation experiment to answer two questions: (a) Does using `glm` and giving rake survey weights calculate standard errors incorrectly? and (b) If so, what R functions can we use to calculate accurate standard errors?  

# Methods

## Simulation

The simulation flow is pictured in the figure below. Each rectangle box represents a fit model. The finite population was set to an *N = 5,000* for each simulation. Two auxiliary variables, *X~1~* and *X~2~*, were drawn from a Bernoulli distribution with a probability of .5; that is, each variable was a dichotomous (0 or 1) variable, with each observation having an equal chance of being 0 or 1. *X~1~* and *X~2~* are always simulated as independent of one another—they are uncorrelated. Two regression coefficients, one for each *X* variable, were drawn from independent uniform distributions ranging from 0 to 1.5. The *X* variables incremented the log odds of the outcome variable of interest *Y* = 1 at the rate of their corresponding coefficient. Lastly, each realization of the *Y* variable was sampled from a Bernoulli distribution where the probabiltiy for each case was transformed from their log odds, mapping onto the assumptions of a logistic regression.  

```{r sim_flow, echo = FALSE, out.width = '70%', fig.align = 'center'}
knitr::include_graphics("simulation_flow.jpg")
```

The first model was fit on the entire population, `glm(y ~ 1, binomial, population_data)`. In all models, an intercept-only binomial logistic regression was fit (`y ~ 1`), which calculates the proportion of cases where *Y* = 1. The population intercept coefficient served as the ground truth. Standard errors in each of the following models will be assessed by how frequently the 95% CI includes this population ground truth. According to the definition of a 95% CI, we will consider standard error calculation methods to be faithfully representing uncertainty if 95% of the CIs contain this population ground truth for a given method.  

The *X* variables form four groups: *X~1~* = *X~2~* = 1; *X~1~* = *X~2~* = 0; *X~1~* = 0 and *X~2~* = 1; *X~1~* = 1 and *X~2~* = 0. A random sample was simulated from this population data, where every memeber in each of these groups had a 2% probability of responding. A model was fit using `glm(y ~ 1, binomial, random_sample)`.  

A sample with non-response bias was then drawn from the population data. If *X~1~* = *X~2~* = 1, the response probabiltiy was set to 6.5%, while the rest of the groups were set to 0.5%. Since the coefficients determining the population values could not be negative, the observations sampled at 6.5% are always more likely to have *Y* = 1; we will expect the non-response to overestimate the estimate. A model was fit to this sample, `glm(y ~ 1, binomial, biased_sample)`, to demonstrate a baseline of how the estimate would be biased if no weighting were done.  

These data with non-response bias were then converted to a survey design object in the `survey` package by using `dat_design <- svydesign(~ 1, data = biased_sample)`, and rake weights were calculated by passing this survey design object to `rake()`. The weighting targets were determined from the population data frame with all 5,000 cases. These weights were assigned to a variable named `wts` in the biased sample data. Three models were fit using these data.  

Weights were passed to the `glm` command as a way of assessing Lumley's (2010) contention that standard errors will be incorrect, `glm(y ~ 1, binomial, biased_sample, wts)`. The `survey` package also includes its own generalized linear models function that employs a sandwich-type estimator for standard errors (see Lumley & Scott, 2017, p. 269). We fit a model using this method, `svyglm(y ~ 1, dat_design, family = binomial)`, to test if this estimator provides a valid 95% CI (note that the `dat_design` object contains the rake weights, so this was a weighted model). Lastly, we passed the `dat_design` object to the `as.svrepdesign` function with `type = "bootstrap"` specified; the resulting object was used with `svyglm()` to calculate bootstrapped standard errors. Bootstrapping involves drawing *R* resamples from the sample, with replacement, and calculating the estimates for each resample. The standard deviation of the estimates across resamples is then considered to be the standard error of the estimate. See Canty and Davison (1999) for the specific bootstrapping implementation used here, and Kline (2004, Ch. 9) for a broader introduction to resampling methods, including boostrapping.  

## Analyses



# Results and Conclusion

```{r, message = FALSE}
results <- read_csv("sim_results.csv")

results %>% 
  select_if(is.double) %>% 
  colMeans() %>% 
  as.matrix()

results %>% 
  select_if(is.double) %>% 
  gather() %>% 
  mutate(key = gsub("_est", "", key),
         key = factor(key, c("pop", "random", "nonrandom", 
                             "wtdglm", "wtdsvy", "wtdrep"))) %>% 
  ggplot(aes(x = value, fill = key)) +
  geom_density(alpha = .5) +
  facet_grid(key ~ .) +
  theme(legend.position = "none")

results %>% 
  select_if(is.logical) %>% 
  colMeans() %>% 
  as.matrix()
```


## Limitations



## References


